{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ● Web crawler-Information of Flickr user's Hometown&Residence MingHsun 2016/01/06\n",
    "### 網路爬蟲是啥?\n",
    "* 網路爬蟲（web crawler），也叫網路蜘蛛（spider），是一種用來自動瀏覽全球資訊網的網路機器人。其目的一般為編纂網路索引。網路搜尋引擎等站點通過爬蟲軟體更新自身的網站內容或其對其他網站的索引。網路爬蟲可以將自己所存取的頁面儲存下來，以便搜尋引擎事後生成索引供用戶搜尋。爬蟲存取網站的過程會消耗目標系統資源。不少網路系統並不默許爬蟲工作。因此在存取大量頁面時，爬蟲需要考慮到規劃、負載，還需要講「禮貌」，或者就是[不被發現]。\n",
    "* 網路爬蟲主要有兩個部分:\n",
    ">1.獲取資訊：在給定一個URL時，我們可以使用HTTP協定的方法去對於伺服器送出Request請求，並且取得Response回應，在我們爬蟲一般的情況就是對於伺服器送出GET Request來取得HTML檔案的Response。<br>\n",
    "2.處理與分析內容：擁有網頁的內容之後，我們就可以透過正規表示、字串處理、split等的各種技巧來過濾抓到我們所要的資料。\n",
    "\n",
    "Reference:https://en.wikipedia.org/wiki/Web_crawler\n",
    "## Python主要模組操作與介紹:<br>\n",
    "\n",
    "###  ● urlib2\n",
    "> urllib.request 是一個用來從URL取得資料的Python模組。它提供了urlopen 這個函數，非常簡單的介面能但他可以接受多種不同的協議。其他關於authentication、cooki等也提供handler或opener等物件操作。以下我們將使用urlopen來取得網站資訊。\n",
    "\n",
    "###  ● BeautifulSoup\n",
    ">   BeautifulSoup是甚麼呢? 簡單來說，它用來parsing html的內容，擷取你想要的HTML tag以及內容資料。網頁就是由一堆HTML tag 階層性的組合而成，BeautifulSoup的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實作-網路爬蟲 Flickr使用者的家鄉、居住地\n",
    "* 我預先已經有一些使用者的ID,Flickr並沒有提供或取使用者居住地的API,但是在使用者的個人資訊網頁上，一般會留家鄉、居住地的資訊，因而我們透過網路爬蟲的方式或取這些資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "uid_file=open(\"UserID.csv\",\"r\")\n",
    "# uid_file=open(\"try_uid.csv\",\"r\")\n",
    "data_dict={}\n",
    "for row in csv.reader(uid_file):\n",
    "#     print row[1]\n",
    "    if row[1] not in data_dict:\n",
    "        data_dict.setdefault(row[1],[])\n",
    "uid_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32762\n"
     ]
    }
   ],
   "source": [
    "print len(data_dict) #總計有幾筆id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling\n",
    "\n",
    "* 1.使用urllib獲取資料\n",
    "* 2.用re套件，正規表示式Regular Expression ，去搜尋家鄉和目前居住地此tag的位置\n",
    "* 3.用Beautifulsoup來parsing資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "for uid in data_dict:\n",
    "    try:\n",
    "        response = urllib2.urlopen('https://www.flickr.com/people/'+str(uid)+\"/\")\n",
    "        html = response.read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        profile_section = soup.find('div', {'id': \"a-bit-more-about\"}) \n",
    "        soup2=BeautifulSoup(str(profile_section), \"html.parser\")\n",
    "        match_h=re.search('家鄉', str(profile_section))\n",
    "        if match_h:\n",
    "            home=soup2.find('dt',text='家鄉︰').findNext('dd').contents\n",
    "#             print home[0]\n",
    "            data_dict[uid].append(home[0])\n",
    "        else:\n",
    "            data_dict[uid].append(\"NoData\")\n",
    "        match_r=re.search('目前居住地', str(profile_section))\n",
    "        if match_r:\n",
    "            residence=soup2.find('dt',text='目前居住地：').findNext('dd').span.span.contents\n",
    "#             print residence[0]\n",
    "            data_dict[uid].append(residence[0])\n",
    "        else:\n",
    "            data_dict[uid].append(\"NoData\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'100028363@N02': ['NoData', 'NoData'], '10000800@N03': ['NoData', 'NoData'], '100004432@N04': ['NoData', 'NoData'], '100030370@N05': ['NoData', 'NoData'], '100004885@N05': ['NoData', 'NoData'], '100011529@N05': ['NoData', 'NoData'], '100013766@N03': ['NoData', 'NoData'], '100015994@N08': ['NoData', 'NoData'], '100018075@N02': ['NoData', 'NoData'], '100009241@N07': [u'\\u9cf3\\u5c71', u'\\u6797\\u5712, \\u53f0\\u7063']}\n"
     ]
    }
   ],
   "source": [
    "print data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 儲存爬蟲結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_list=[[\"uid\",\"home\",\"residence\"]]\n",
    "for uid in data_dict:\n",
    "    csv_list.append([str(uid),data_dict[uid][0].encode(\"utf8\"),data_dict[uid][1].encode(\"utf8\")])\n",
    "home_f=open(\"home_match.csv\",\"w\")\n",
    "w = csv.writer(home_f)  \n",
    "w.writerows(csv_list)  \n",
    "home_f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
