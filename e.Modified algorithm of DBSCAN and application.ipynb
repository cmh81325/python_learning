{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ● 機器學習演算法-DBSCAN 修改為 P-DBSCAN與應用 MingHsun 2016/03/15\n",
    "* 主要原因:應用DBSCAN於大量的Flickr 照片資料，找尋地理空間上高密度拍照與人群聚集之地區，* 然而在Flickr資料中經常有，一位使用者有多張拍照的問題' ，因而將照片數量作為DBSCAN minpoint參數會形成偏誤，因此自行修改DBSCAN之演算法為P-DBSCAN，以拍照者數量作為minpoint參數。\n",
    "* 應用:\n",
    "> 1.讀取Shapefile資料中點位資料<br>\n",
    "  2.應用修改DBSCAN後的P-DBSCAN於點資料中以找尋clustering<br>\n",
    "  3.使用convexhull繪製這些clustering的邊界<br>\n",
    "  4.將這些邊界輸出成Shapefile\n",
    "  \n",
    "在本篇中還包含了pygmal與picke的說明與使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.讀取Shapefile資料中點位資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import shapefile\n",
    "dataset=[]\n",
    "# y_list=[2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015]\n",
    "y_list=[2012]\n",
    "for yy in y_list:\n",
    "    year=str(yy)\n",
    "    sf = shapefile.Reader(\"clip/\"+year+\"clip.shp\")\n",
    "    shapes = sf.shapes()\n",
    "    fields = sf.fields\n",
    "    records = sf.records()\n",
    "    for attribute in records:\n",
    "        dataset.append((attribute[7],attribute[8],attribute[1],attribute[5],attribute[4]))\n",
    "print (\"原始資料數量\",len(dataset))\n",
    "check_dataset_list=list(set(dataset))\n",
    "print (\"排除同人同地點重複\",len(check_dataset_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. P-DBSCAN Algorithm\n",
    "*DBSCAN主要特點為能夠跨越邊界的限制，排除低密度的雜訊點位 (Noise Point) 以找處高密度的群集點位 -要搜尋半徑範圍 (Radius) 與形成核心群集的資料點數量門檻值 (Min-Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#參數\n",
    "D=check_dataset_list\n",
    "eps=10\n",
    "min_owner=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regionQuery(p):\n",
    "     return  (((p[0]-p_x)**2+ (p[1]-p_y)**2)**0.5)<=eps\n",
    "    \n",
    "# def ownercheck(regionQ_list):\n",
    "#     check_list=regionQ_list\n",
    "#     neighborID_list=[]\n",
    "#     for pp in check_list:\n",
    "#         if pp[2] not in neighborID_list:\n",
    "#             neighborID_list.append(pp[2])\n",
    "#     return neighborID_list\n",
    "def ownercheck(p):\n",
    "    neighbor_ID_list.append(p[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499775413.5376687\n",
      "1499775709.3765898\n",
      "295.8389210700989\n"
     ]
    }
   ],
   "source": [
    "noise=[]\n",
    "visited=[]\n",
    "cluster={}\n",
    "cluster_count=-1\n",
    "f=time.time()\n",
    "print (f)\n",
    "for point in D:\n",
    "    if point not in visited:\n",
    "        visited.append(point)\n",
    "        p_x=point[0]\n",
    "        p_y=point[1]\n",
    "        neighbor_plist = filter(regionQuery,D)\n",
    "        neighbor_ID_list=[]\n",
    "        map(ownercheck,neighbor_plist)\n",
    "        neighbor_num=len(set(neighbor_ID_list))\n",
    "        if neighbor_num < min_owner:\n",
    "            noise.append(point)\n",
    "        else:\n",
    "            cluster_count=cluster_count+1\n",
    "            name = 'cluster'+str(cluster_count);\n",
    "            cluster.setdefault(name,[])\n",
    "            cluster[name].append(point)\n",
    "            \n",
    "            for p in neighbor_plist:\n",
    "                if p not in visited:\n",
    "                    visited.append(p)\n",
    "                    if p not in cluster[name]:\n",
    "                        cluster[name].append(p)\n",
    "                    p_x=p[0]\n",
    "                    p_y=p[1]\n",
    "                    n_plist=filter(regionQuery,D)\n",
    "                    neighbor_ID_list=[]\n",
    "         \n",
    "                    map(ownercheck,n_plist)\n",
    "                    n_num=len(set(neighbor_ID_list))\n",
    "                    if n_num>=min_owner:\n",
    "                        \n",
    "                        sub=set(n_plist)-set(neighbor_plist)\n",
    "                        sub_list=list(sub)\n",
    "                        for n in sub_list:\n",
    "                            neighbor_plist.append(n)\n",
    "\n",
    "e=time.time()\n",
    "print (e)\n",
    "print (e-f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.將P-DBSCAN 分析成果，使用convexhull繪製集群邊界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_number 0\n",
      "noise 59273\n"
     ]
    }
   ],
   "source": [
    "#統計集群 並且將集群進行append 以方便後續convexhull計算\n",
    "convex_dict={}\n",
    "c=0\n",
    "print (\"cluster_number\",len(cluster))\n",
    "for aa in cluster:\n",
    "    convex_dict.setdefault(aa,[])\n",
    "    c=c+1\n",
    "    for bb in cluster[aa]:\n",
    "#         c=c+1\n",
    "        convex_dict[aa].append([float(bb[4]),float(bb[3])])\n",
    "    print (aa,len(cluster[aa]))\n",
    "# print \"all\",c\n",
    "print (\"noise\",len(noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#計算convexhull\n",
    "def vertice_value(v):\n",
    "    convex_plist.append(tuple(convex_dict[aa][v]))\n",
    "convex_pdict={}\n",
    "from scipy.spatial import ConvexHull\n",
    "for aa in convex_dict:\n",
    "    convex_plist=[]\n",
    "    hull=ConvexHull(convex_dict[aa])\n",
    "    map(vertice_value,hull.vertices)\n",
    "    convex_plist.append(tuple(convex_plist[0]))\n",
    "    convex_pdict.setdefault(aa,convex_plist)\n",
    "# print convex_pdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## module-pygmaps\n",
    "* 一個可以客製化繪製地理資料於Google Map的模組<BR>\n",
    "* 將上述的成果繪製到地圖上呈現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#畫圖\n",
    "import pygmaps\n",
    "import random\n",
    "lat, lng = 25.05565, 121.56\n",
    "mymap = pygmaps.maps(lat, lng, 13)\n",
    "# color_list=[]\n",
    "#繪製點位\n",
    "for c in cluster:\n",
    "    color = \"#%06x\" % random.randint(0, 0xFFFFFF)\n",
    "    for cp in cluster[c]:\n",
    "        y=float(cp[4])\n",
    "        x=float(cp[3])\n",
    "        mymap.addpoint(y,x,color,str(c))\n",
    "\n",
    "all_con_list=[]\n",
    "#繪製convehull\n",
    "try:\n",
    "    for con in convex_pdict:\n",
    "        conlist=convex_pdict[con]\n",
    "\n",
    "        all_con_list.append(conlist)\n",
    "        mymap.addpath(conlist,\"#00FF00\")\n",
    "except:\n",
    "    print (error)\n",
    "    \n",
    "# # #繪製noise\n",
    "# for n in noise:\n",
    "#     y=float(n[4])\n",
    "#     x=float(n[3])\n",
    "#     mymap.addpoint(y,x,\"#F6E3CE\",\"noise\")\n",
    "\n",
    "    \n",
    "mymap.draw('p-dbscan.html')\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=dbscan.html width=1000 height=2000></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## module-pickle\n",
    "* 除了基本I/O來保存運算結果之外，物件序列化（Serialization)，就是如果想直接保存物件狀態，在下次重新執行程式時讀取以恢復運算時必要的資料，這類的技術稱為物件序列化（Object serialization）。\n",
    "* Q:目的為為何呢?A:方便存取使用\n",
    "* 像在P-DBSCAN的這個運算當中，由於資料量龐大且此演算法是一個較耗時的演算法，因此以pickle來存取運算的結果，以方便後續使用。\n",
    "\n",
    "\n",
    "pickle兩個主要函數\n",
    ">dump()  進行物件序列化<br>\n",
    " load() 載入pickle檔<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "convex_pdict_p= convex_pdict\n",
    "pickle.dump(convex_pdict_p, open( \"convex_pdict_p.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster0': [(25.0346, 121.565), (25.033588, 121.565351), (25.033078, 121.564944), (25.032883, 121.564385), (25.033272, 121.563999), (25.033559, 121.563721), (25.034249, 121.563914), (25.034438, 121.564171), (25.0346, 121.565), '#00FF00']}\n"
     ]
    }
   ],
   "source": [
    "print convex_pdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.GIS-GDAL\n",
    "*  繪製convexhull to polygon(存放至:cluster_dhp資料夾)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import osgeo.ogr as ogr\n",
    "import osgeo.osr as osr\n",
    "for clust in convex_pdict:\n",
    "    outSHPfn = \"cluster_shp/\"+clust+\".shp\"\n",
    "    # set up the shapefile driver\n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "\n",
    "    #delete shp\n",
    "    if os.path.exists(outSHPfn):\n",
    "        driver.DeleteDataSource(outSHPfn)\n",
    "    # create the data source\n",
    "    data_source = driver.CreateDataSource(outSHPfn)\n",
    "    # create the spatial reference, WGS84\n",
    "    srs = osr.SpatialReference()\n",
    "#     srs.ImportFromEPSG(4326)\n",
    "    srs.SetWellKnownGeogCS( \"WGS84\" );\n",
    "#     spatialReference = osr.SpatialReference()\n",
    "#     spatialReference.ImportFromProj4('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs')\n",
    "    # create the layer\n",
    "    # layer = data_source.CreateLayer(outSHPfn,srs, ogr.wkbPoint)\n",
    "    layer = data_source.CreateLayer(outSHPfn,srs, ogr.wkbPolygon)\n",
    "\n",
    "    # create the feature\n",
    "    feature = ogr.Feature(layer.GetLayerDefn())\n",
    "\n",
    "    # # Add the fields we're interested in\n",
    "    # field_photoID = ogr.FieldDefn(\"Name\", ogr.OFTString)\n",
    "    # field_photoID.SetWidth(24)\n",
    "    # layer.CreateField(field_photoID)\n",
    "    # Create ring\n",
    "    ring = ogr.Geometry(ogr.wkbLinearRing)\n",
    "    for i in range(0,len(convex_pdict[clust])-1):\n",
    "#         print convex_pdict[clust][i][1],convex_pdict[clust][i][0]\n",
    "        ring.AddPoint(convex_pdict[clust][i][1],convex_pdict[clust][i][0])\n",
    "\n",
    "    poly = ogr.Geometry(ogr.wkbPolygon)\n",
    "    poly.AddGeometry(ring)\n",
    "    # Set the feature geometry using the point\n",
    "    feature.SetGeometry(poly)\n",
    "    # Create the feature in the layer (shapefile)\n",
    "    layer.CreateFeature(feature)\n",
    "    # Destroy the feature to free resources\n",
    "    feature.Destroy()\n",
    "    # Destroy the data source to free resources\n",
    "    data_source.Destroy()\n",
    "#     print \"-----\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
